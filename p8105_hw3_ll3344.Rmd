---
title: "p8105_hw3_ll3344"
author: "Lusha Liang"
output: github_document
---

First, load all necessary libraries:

```{r}
library(tidyverse)
library(p8105.datasets)
library(patchwork)

knitr:: opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.color = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1

Load the instacart dataset from the p8105 datasets.
```{r}
data("instacart")
```

* The instacart dataset essentially shows online grocery store purchases from different users (user_id) and shows the particular item purchased (product_name) as well as the time of day the item was purchased (order_hour_of_day), the day of the week (order_dow), and the aisle to which the product belongs (aisle). The dataset also contains information on whether or not the product was a re-order (reordered), among other information.
* The dataset contains `r nrow(instacart)` observations. Each row in the dataset is a product from an order. There is a single order per user in this dataset.
* There are `r ncol(instacart)` variables. Variables included are: `r colnames(instacart)`. 
* To give an example, row 1 shows a purchase by a customer with User ID # `r instacart %>% slice_head() %>% pull(user_id)` who is reordering `r instacart %>% slice_head() %>% pull(product_name)` at `r instacart %>% slice_head() %>% pull(order_hour_of_day)` AM. 

Now let's figure out how many aisles are represented in the dataset:
```{r}
instacart %>%
  count(aisle) %>%
  arrange(desc(n))
```
There are 134 aisles. The top 5 most ordered from aisles are fresh vegetables, fresh fruits, packaged vegetables/fruits, yogurt, and packaged cheese.

Now let's make a plot that shows the number of items ordered in each aisle, limiting to aisles with more than 10,000 items ordered. 
```{r}
instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>%
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  xlab("Aisle name") + 
  ylab("Number of items ordered") +
  ggtitle("Most popular aisles in Instacart")
```
The aisles are ordered on the X-axis from left to right by increasing number of items ordered.

We will now make a table featuring the three most popular items in the aisles: "baking ingredients", "dog food care", and "packaged vegetables fruits."
```{r}
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>%
  count(product_name) %>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(aisle, rank) %>%
  knitr::kable()
```

Finally, we will make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.
```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>%
  knitr::kable()
```

## Problem 2

Load accelerometer data.
```{r}
accel_df = 
  read_csv("./data/accel_data.csv") 
```

Clean and wrangle the data. 
```{r}
accel_df = 
  accel_df %>%
  janitor::clean_names() %>%
  mutate(weekend = ifelse(day %in% c("Saturday", "Sunday"), "weekend", "weekday")) %>%
  relocate(week, day_id, day, weekend) %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute_of_day",
    names_prefix = "activity_",
    values_to = "activity"
  ) %>%
  mutate(day = factor(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")),
         weekend = factor(weekend),
         minute_of_day = as.numeric(minute_of_day),
         ) %>%
  group_by(week) %>%
  arrange(day, .by_group = TRUE)
```

* There are a total of `r nrow(accel_df)` observations and `r ncol(accel_df)` variables. 
* The 6 variables are: `r colnames(accel_df)`.

We will now sum total minutes of activity for each day and create a table showing these totals. 
```{r}
accel_df %>%
  group_by(day, week) %>%
  summarize(total_activity = sum(activity)) %>%
  pivot_wider(
    names_from = week,
    values_from = total_activity
  ) %>%
  knitr::kable()
```
It seems that on average the patient has less activity on Saturdays and more on Fridays.

We will now plot activity over the course of the day. 
```{r}
accel_df %>%
  ggplot(aes(x = minute_of_day,
             y = activity,
             color = day)) +
  geom_line(alpha =.5) +
  geom_smooth() +
  xlab("Minute of the day") + 
  ylab("Activity") +
  ggtitle("Activity throughout the week") + 
  scale_color_discrete("Day of the week")
```
The patient has little activity between minutes 0 to about 400. This corresponds to the hours between midnight and 7am, during which time he is probably sleeping. His most active periods during the week appear to be Sunday mornings and Friday evenings. 

## Problem 3

Load the noaa dataset.
```{r}
data("ny_noaa")
```

* This dataset contains information from the NOAA (National Oceanic and Atmospheric Association) National Climatic Data Center for all New York state weather stations from January 1, 1981 through December 31, 2010. 
* It contains `r nrow(ny_noaa)` observations of `r ncol(ny_noaa)` variables. 
* Essentially for each day from January 1, 1981-December 31, 2010 the dataset contains information on the amount of precipitation in tenths of mm (prcp), snowfall in mm (snow), snow depth in mm (snwd), as well as the maximum (tmax) and minimum temperature (tmin) in Celsius. 
* Since each station only collects a subset of this dat, the resulting dataset does contain a significant amount of missing data shown in the below table by column/variable. 

```{r}
ny_noaa %>%
  summarise_all(funs(sum(is.na(.))))
```

Now we will clean the data.
```{r}
noaa_df = 
  ny_noaa %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(tmax = as.numeric(tmax)/10,
         tmin = as.numeric(tmin)/10,
         year = as.factor(year),
         month = as.integer(month),
         day = as.integer(day)
         ) 
```
The units for precipitation and snowfall/snow depth have remained the same but we have converted the temperatures into Celsius rather than tenths of degrees Celsius. 

```{r}
noaa_df %>%
  count(snow) %>%
  arrange(desc(n)) %>%
  head()
```
The top 5 most commonly observed values for snowfall are 0 mm, NA, 25 mm, 13 mm, and 51 mm. The numbers may seem somewhat arbitrary but make more sense when we consider that snowfall was likely measured in inches, then converted into mm. 13mm ~ 0.5 inch, 25 mm ~ 1 inch, 51 mm ~ 2 inches, and so on. 

We will now make a two-panel plot showing the average max temperature in each station across years. Is there any observable / interpretable structure? Any outliers?
```{r}
noaa_df %>%
  filter(month == 1 | month == 7) %>%
  drop_na() %>%
  group_by(id, year, month) %>%
  summarize(mean_max = mean(tmax)) %>%
  ggplot(aes(x = year, y = mean_max, group = id)) +
  geom_point() +
  geom_path() +
  facet_grid(~month) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.
```{r}
tmax_tmin_p = 
  noaa_df %>%
  ggplot(aes(x = tmin, y = tmax)) +
  geom_bin2d()

snowfall_p = 
  noaa_df %>%
  filter(snow > 0, snow < 100) %>%
  ggplot(aes(x = year, y = snow)) +
  geom_violin(aes(fill = year))

tmax_tmin_p/snowfall_p
```

